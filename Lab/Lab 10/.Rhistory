var_Cyclopropane_death_proportion
sum(weight^2*(var_Halothane_death_proportion+var_Cyclopropane_death_proportion))
sum(weight^2*(var_Halothane_death_proportion+var_Cyclopropane_death_proportion))/sum(weight)^2
var_difference = sum(weight^2*(var_Halothane_death_proportion+var_Cyclopropane_death_proportion))/sum(weight)^2
(1809 - 2684)/sqrt(var_difference)
sqrt(var_difference)
# Standardized death rate for Halothane
Halothane_standardized_rate <- sum(weight*Halothane_death_proportion)/sum(weight) * 100000
# standardized death rate for Cyclopropane
Cyclopropane_standardized_rate <- sum(weight*Cyclopropane_death_proportion)/sum(weight) * 100000
# Standardized death rate for Halothane
Halothane_standardized_rate <- sum(weight*Halothane_death_proportion)/sum(weight)
Halothane_standardized_rate * 100000
# standardized death rate for Cyclopropane
Cyclopropane_standardized_rate <- sum(weight*Cyclopropane_death_proportion)/sum(weight)
Cyclopropane_standardized_rate * 100000
(Halothane_standardized_rate - Cyclopropane_standardized_rate)/sqrt(var_difference)
Q1 <- Q1 %>%
mutate(total_death_proportion = Total_Deaths/Total_Operations)
attach(Q1)
study_proportion <- sum(Halothane_Deaths)/sum(Halothane_Operations)
study_proportion
reference_proportion <- sum(Halothane_Operations*total_death_proportion)/sum(Halothane_Operations)
reference_proportion
sum(Halothane_Deaths)/sum(Halothane_Operations*total_death_proportion)
study_proportion/reference_proportion
s = study_proportion/reference_proportion
s
var(O) = sum(Halothane_Deaths)
var_O = sum(Halothane_Deaths)
var_E = sum((Halothane_Operations/Total_Operations)^2*Total_Deaths)
E = sum(Halothane_Operations*total_death_proportion)
s = study_proportion/reference_proportion
s
var_s = (var_O + s^2*var_E)/E^2
var_s
(s-1)/sqrt(var_s)
44 + 138 + 130 + 102
study_proportion
reference_proportion <- sum(Halothane_Operations*total_death_proportion)/sum(Halothane_Operations)
reference_proportion
study_proportion
reference_proportion
Q2 <- read.table('Homework 8_SURV.txt', col.names = c('ID', 'TIMEDEATH', 'DEATH', 'AGE', 'GROUP'))
View(Q2)
library(survival)
x <- rep(1,110)
Q2 <- read.table('Homework 8_SURV.txt', col.names = c('ID', 'TIMEDEATH', 'DEATH', 'AGE', 'GROUP'))
attach(Q2)
fit <- survfit(Surv(TIMEDEATH,DEATH)~GROUP, conf.type = "none")
plot(fit,xlab="t",ylab="S(t)",lwd=c(1,3))
legend(25,1,c("New Treatment","Placebo"),lwd=c(3,1))
plot(fit,xlab="t",ylab="S(t)",lwd=c(1,3))
plot(fit,xlab="t",ylab="S(t)",lwd=c(1,3)) + legend(25,1,c("New Treatment","Placebo"),lwd=c(3,1))
plot(fit,xlab="t",ylab="S(t)",lwd=c(1,3))
legend(25,1,c("New Treatment","Placebo"),lwd=c(3,1))
legend(25,1,c("New Treatment","Placebo"),lwd=c(3,1))}
{plot(fit,xlab="t",ylab="S(t)",lwd=c(1,3)),
plot(fit,xlab="t",ylab="S(t)",lwd=c(1,3))
legend(25,1,c("New Treatment","Placebo"),lwd=c(3,1))
plot(fit,xlab="t",ylab="S(t)",lwd=c(1,3))
plot(fit,xlab="t",ylab="S(t)")
plot(fit,xlab="t",ylab="S(t)",lwd=c(1,3))
fit
plot(fit,xlab="t",ylab="S(t)",lwd=c(1,3))
legend(500,1,c("New Treatment","Placebo"),lwd=c(3,1))
plot(fit,xlab="t",ylab="S(t)",lwd=c(1,3))
legend(500,1,c("New Treatment","Placebo"),lwd=c(3,1))
plot(fit,xlab="t",ylab="S(t)",lwd=c(1,3))
plot(fit,xlab="t",ylab="S(t)",lwd=c(1,3))
legend(500,0.8,c("New Treatment","Placebo"),lwd=c(3,1))
plot(fit,xlab="t",ylab="S(t)",lwd=c(1,3))
legend(400,1,c("New Treatment","Placebo"),lwd=c(3,1))
survdiff(Surv(TIMEDEATH, DEATH)~GROUP)
summary(coxph(Surv(TIMEDEATH, DEATH)~GROUP))
summary(coxph(Surv(TIMEDEATH, DEATH)~GROUP))
summary(coxph(Surv(TIMEDEATH, DEATH)~GROUP+AGE))
coxph(Surv(TIMEDEATH, DEATH)~GROUP+AGE)
survfit(Surv(TIMEDEATH,DEATH)~GROUP, conf.type = "none")
survfit(Surv(TIMEDEATH,DEATH)~GROUP+DEATH)
survfit(Surv(TIMEDEATH,DEATH)~GROUP+DEATH, conf.type = "none")
survfit(Surv(TIMEDEATH,DEATH)~GROUP, conf.type = "none")
survfit(Surv(TIMEDEATH,DEATH)~GROUP)
survfit(Surv(TIMEDEATH,DEATH)~GROUP+AGE, conf.type = "none")
survfit(Surv(TIMEDEATH,DEATH)~GROUP+AGE)
survfit(Surv(TIMEDEATH,DEATH)~GROUP, conf.type = "none")
attach(WorldPhones)
head(WOrldPhones)
head(WorldPhones)
WorldPhones
View(WOrldPones)
View(WorldPones)
View(WorldPhones)
names(WorldPhones)
?WorldPhones
?Gather
?gather
WorldPhones %>%
gather(key = continent, value = number, n.Amer, Europe, Asia, S.Amer, Oceania, Africa, Mid.Amer)
WorldPhones %>%
gather(key = continent, value = number, c(n.Amer, Europe, Asia, S.Amer, Oceania, Africa, Mid.Amer))
mini_iris <- iris[c(1, 51, 101), ]
gather(mini_iris, key = flower_att, value = measurement,
Sepal.Length, Sepal.Width, Petal.Length, Petal.Width)
gather(WorldPhones, key = continent, value = number, n.Amer, Europe, Asia, S.Amer, Oceania, Africa, Mid.Amer)
WorldPhones[1,]
gather(WorldPhones, key = continent, value = number, -WorldPhones[,1])
gather(WorldPhones, key = continent, value = number, WorldPhones$N.Amer, WorldPhones$Europe)
gather(WorldPhones, key = continent, value = number, `n.Amer`, `Europe`, `Asia`, `S.Amer`, `Oceania`, `Africa`, `Mid.Amer`)
gather(WorldPhones, key = continent, value = number, "n.Amer", "Europe", "Asia", "S.Amer", "Oceania", "Africa", "Mid.Amer")
gather(WorldPhones, key = "continent", value = "number", "n.Amer", "Europe", "Asia", "S.Amer", "Oceania", "Africa", "Mid.Amer")
gather(WorldPhones, "n.Amer", "Europe", "Asia", "S.Amer", "Oceania", "Africa", "Mid.Amer", key = "continent", value = "number")
dat <-
WorldPhones %>%
data.frame() %>%
add_rownames("year") %>%
gather(key = "continent", value = "phones", -year)
dat
dat <-
WorldPhones %>%
data.frame() %>%
add_rownames("year") %>%
mutate(year = as.integer(year))
dat %>%
ggplot(aes(year, phones, col=continent)) +
geom_point()
dat <-
WorldPhones %>%
data.frame() %>%
add_rownames("year") %>%
mutate(year = as.integer(year))
library(tidyverse)
dat <-
WorldPhones %>%
data.frame() %>%
add_rownames("year") %>%
mutate(year = as.integer(year))
dat <-
WorldPhones %>%
data.frame() %>%
add_rownames("year") %>%
mutate(year = as.integer(year)) %>%
gather(key = "continent", value = "phones", -year)
lm.1 <- lm(phones ~ year*continent, dat)
WorldPhones %>%
data.frame() %>%
add_rownames("year") %>%
mutate(year = as.integer(year)) %>%
gather(key = "continent", value = "phones", -year)
dat %>%
add_predictions(lm.1) %>%
ggplot(aes(year, phones, col=continent)) +
geom_point() +
geom_line(aes(year, pred, col=continent))
dat %>%
add_predictions(lm.1) %>%
ggplot(aes(year, phones, col=continent)) +
geom_point() +
geom_line(aes(year, pred, col=continent))
lm(phones ~ year*continent, dat)
summary(lm(phones ~ year*continent, dat))
library(caret)
## a. partitioning data
set.seed(1)
index <- createDataPartition(iris$Species, p=0.75, list=FALSE)
tr <- iris[index,]
te <- iris[-index,]
## a. standardizing data
preProcValues <- preProcess(iris, method = c("knnImpute","center","scale"))
iris <- predict(preProcValues, iris)
## b. Feature selection rfe = recursive feature elimination
set.seed(1)
RFE <- rfe(x = tr %>% select(-Species),
y = tr$Species,
rfeControl = rfeControl(functions = rfFuncs, method = "repeatedcv",
repeats = 3, verbose = FALSE))
x <- RFE$optVariables[1:4]
y <- "Species"
# K-Nearest Neighbors
set.seed(1)
model.knn <- train(tr[, x], tr[, y], method = "knn")
# Random forest
set.seed(1)
model.rf <- train(tr[, x], tr[, y], method = "rf")
# Neural Network
set.seed(1)
model.nn <- train(tr[, x], tr[, y], method = "nnet")
## c. Prediction
pred_knn <- predict.train(object = model.knn, te[, x], type = "raw")
table(pred_knn)
confusionMatrix(pred_knn, te[, y])
pred_rf <- predict.train(object = model.rf, te[, x], type = "raw")
table(pred_rf)
confusionMatrix(pred_rf, te[, y])
pred_nn <- predict.train(object = model.nn, te[, x], type = "raw")
table(pred_nn)
confusionMatrix(pred_nn, te[, y])
Q2 <- read.table('Homework 8_SURV.txt', col.names = c('ID', 'TIMEDEATH', 'DEATH', 'AGE', 'GROUP'))
attach(Q2)
library(survival)
fit <- survfit(Surv(TIMEDEATH,DEATH)~GROUP, conf.type = "none")
plot(fit,xlab="t",ylab="S(t)",lwd=c(1,3))
legend(400,1,c("New Treatment","Placebo"),lwd=c(3,1))
Q1 <- Q1 %>%
mutate(total_death_proportion = Total_Deaths/Total_Operations)
attach(Q1)
study_proportion <- sum(Halothane_Deaths)/sum(Halothane_Operations)
study_proportion
reference_proportion <- sum(Halothane_Operations*total_death_proportion)/sum(Halothane_Operations)
reference_proportion
s = study_proportion/reference_proportion
s
var_O = sum(Halothane_Deaths)
var_E = sum((Halothane_Operations/Total_Operations)^2*Total_Deaths)
E = sum(Halothane_Operations*total_death_proportion)
var_s = (var_O + s^2*var_E)/E^2
var_s = (var_O + s^2*var_E)/E^2
var_s
library(tidyverse)
Q1 <- read.table('Homework 8_Q1.txt', col.names = c('Physical_Status', 'Total_Operations', 'Halothane_Operations', 'Cyclopropane_Operations', 'Total_Deaths', 'Halothane_Deaths', 'Cyclopropane_Deaths'))
View(Q1)
Q1 <- read.table('Homework 8_Q1.txt', col.names = c('Physical_Status', 'Total_Operations', 'Halothane_Operations', 'Cyclopropane_Operations', 'Total_Deaths', 'Halothane_Deaths', 'Cyclopropane_Deaths'))
# Halothane crude death rate
sum(Q1$Halothane_Deaths)/sum(Q1$Halothane_Operations)
# Cyclopropane crude death rate
sum(Q1$Cyclopropane_Deaths)/sum(Q1$Cyclopropane_Operations)*100000
sum(Q1$Halothane_Deaths)/sum(Q1$Halothane_Operations)
sum(Q1$Cyclopropane_Deaths)/sum(Q1$Cyclopropane_Operations)
(0.01624-0.03094-0)/sqrt{0.01624*(1-0.01624)/sum(Q1$Halothane_Operations) + 0.03094*(1-0.03094)/sum(Q1$Cyclopropane_Operations)}
(0.01624-0.03094-0)/sqrt(0.01624*(1-0.01624)/sum(Q1$Halothane_Operations) + 0.03094*(1-0.03094)/sum(Q1$Cyclopropane_Operations))
sqrt(0.01624*(1-0.01624)/sum(Q1$Halothane_Operations) + 0.03094*(1-0.03094)/sum(Q1$Cyclopropane_Operations))
(0.01624-0.03094-0)
pnorm(-19.83, 0.95)
?pnorm
qnorm(0.95)
qnorm(0.975)
pnorm(1.95)
pnorm(-19)
pnorm(-19.83)
s
reference_proportion
study_proportion
s
# Set the random seed
set.seed(0)
# Generate example data set
N = 30
U1 = 1
U2 = 4.5
gene_df = rbind(tibble(gene_a = rnorm(n=N, mean=U2), gene_b = rnorm(n=N, mean=U1), explant_id = seq(1, N), type = 1),
tibble(gene_a = rnorm(n=N, mean=U1), gene_b = rnorm(n=N, mean=U1), explant_id = seq(N+1, 2*N), type = 2),
tibble(gene_a = rnorm(n=N, mean=U1), gene_b = rnorm(n=N, mean=U2), explant_id = seq((2*N)+1, 3*N), type = 3))
gene_df$type = as.factor(gene_df$type)
ggplot(gene_df, aes(gene_a, gene_b)) +
geom_point(size=2, alpha=0.75)
gene_df
gene_df %>%
select(gene_a, gene_b) %>%
kmeans(3)
#-------------------------
# K-means clustering
#-------------------------
fit = gene_df %>%
select(gene_a, gene_b) %>%
kmeans(3)
# Append cluster assignment
gene_df$cluster = as.factor(fit$cluster)
gene_df
# Plot results
ggplot(gene_df, aes(gene_a, gene_b, group=cluster)) +
geom_point(size=2, alpha=0.75, aes(color=cluster))
#-------------------------
# Determine an appropriate
# number of clusters for the
# data
#-------------------------
kmeans_wss <- Vectorize(function(c){
return(sum(kmeans(gene_df, centers=c)$withinss))
})
cluster_fit = tibble(num_centers=1:15) %>%
mutate(wss = kmeans_wss(num_centers), diff_wss = wss - lag(wss))
ggplot(cluster_fit, aes(num_centers, wss)) +
geom_point() +
geom_line() +
labs(x="Number of Clusters",
y="Within groups sum of squares")
tibble(num_centers=1:15)
cluster_fit = tibble(num_centers=1:15) %>%
mutate(wss = kmeans_wss(num_centers), diff_wss = wss - lag(wss))
cluster_fit
# Expected to be from type 2
new_samples = tibble(gene_a = rnorm(n=10, mean=1), gene_b = rnorm(n=10, mean=1))
new_samples
# Train a k-nearest-neighbors model
knn_fit = train(type ~ gene_a + gene_b, data = gene_df, method = "knn")
knn_fit
# Predict type for new data
knn_classification = predict(knn_fit, new_samples, type="raw")
knn_classification
# Train a support vector machine
svm_fit = train(type ~ gene_a + gene_b, data = gene_df, method = "svmLinear")
# Train a support vector machine
svm_fit = train(type ~ gene_a + gene_b, data = gene_df, method = "svmLinear")
# Predict type for new data
svm_classification = predict(svm_fit, new_samples, type="raw")
svm_classification
police_df = read_csv(url('https://raw.githubusercontent.com/fivethirtyeight/data/master/police-locals/police-locals.csv'), na = '**')
View(police_df)
#-----------------------------------
# Standardize
#-----------------------------------
preproc_vals = police_df %>%
preProcess(method = c("center", "scale"))
police_standardized = predict(preproc_vals, police_df)
#-----------------------------------
# Impute missing values
#-----------------------------------
preproc_vals = police_df %>%
preProcess(method = c("knnImpute"))
police_imputed = predict(preproc_vals, police_df)
# First, notice how halfing one variable or another
# impacts the distance between observations
astronaut_df = tibble(astronaut_id = c(13, 45, 81),
age = c(36, 47, 38),
heart_rate = c(45, 52,42),
hours_in_space = c(2208, 3791, 1823))
# Exclude astronaut IDs
adf_0 = astronaut_df %>% select(-astronaut_id)
d0 = dist(adf_0)
adf_1 = adf_0
adf_1[1, 3] = 0.5 * adf_0[1, 3]
d1 = dist(adf_1 )
delta1 = (d1 - d0)/d0
adf_2 = adf_0
adf_2[1, 2] = 0.5 * adf_0[1, 2]
d2 = dist(adf_2)
delta2 = (d2 - d0)/d0
# Now standardize the data by converting to z-scores
# Test the distance scores again
astronaut_stnd_df = astronaut_df %>%
mutate(age = (age - mean(age))/sd(age),
heart_rate = (heart_rate - mean(heart_rate)) / sd(heart_rate),
hours_in_space = (hours_in_space - mean(hours_in_space)) / sd(hours_in_space))
adf_3 = astronaut_stnd_df %>% select(-astronaut_id)
d3 = dist(adf_3)
adf_4 = adf_3
adf_4[1, 3] = 0.5 * adf_3[1, 3]
d4 = dist(adf_4)
delta4 = (d4 - d3)/d3
adf_5 = adf_3
adf_5[1, 2] = 0.5 * adf_3[1, 2]
d5 = dist(adf_5)
delta5 = (d5 - d3)/d3
## Now let's standardize using caret
astro_preproc_vals = adf_0 %>%
preProcess(method = c("center", "scale"))
astronaut_stnd_df_2 = predict(astro_preproc_vals, astronaut_df)
## Curse of dimensionality
# Add three new variables
astronaut_df$weight = c(172, 169, 201)
astronaut_df$height = c(69, 64, 73)
astronaut_df$systolic = c(124, 118, 129)
adf_6 = astronaut_df %>% select(-astronaut_id)
# Standardize
astro_preproc_vals = adf_6 %>%
preProcess(method = c("center", "scale"))
astronaut_stnd_df_3 = predict(astro_preproc_vals, astronaut_df)
# Check the effect on the distance metric of halving a variable
adf_7 = astronaut_stnd_df_3 %>% select(-astronaut_id)
d7 = dist(adf_7)
adf_8 = adf_7
adf_8[1, 3] = 0.5 * adf_7[1, 3]
d8 = dist(adf_8)
delta8 = (d8 - d7)/d7 # Compare to delta4
## Curse of dimensionality
# Add 500 new variables
astronaut_stnd_df_4 = astronaut_stnd_df_3
astronaut_stnd_df_4 = cbind(astronaut_stnd_df_4, matrix(rnorm(1500), ncol=500))
adf_9 = astronaut_stnd_df_4 %>% select(-astronaut_id)
adf_9
View(adf_9)
## 2. data
dat.raw <- read.csv("lab10data.csv",stringsAsFactors = T)
dim(dat)
sum(complete.cases(dat))
## 2.1 imputation (single imputation) fill in missing valeu with computed values?
preProcValues <- preProcess(dat.raw, method = c("knnImpute","center","scale")) # every covariate has mean of 0 and st of 1
dat <- predict(preProcValues, dat.raw)
dim(dat)
sum(complete.cases(dat))
## 2. data
dat.raw <- read.csv("lab10data.csv",stringsAsFactors = T)
setwd("~/School/BIOS 611/BIOS611Git/Lab/Lab 10")
library(caret)
library(tidyverse)
## 2. data
dat.raw <- read.csv("lab10data.csv",stringsAsFactors = T)
dim(dat.raw)
sum(complete.cases(dat.raw))
## 2.1 imputation (single imputation) fill in missing valeu with computed values?
preProcValues <- preProcess(dat.raw, method = c("knnImpute","center","scale")) # every covariate has mean of 0 and st of 1
dat <- predict(preProcValues, dat.raw)
dim(dat)
sum(complete.cases(dat))
## 3. partitioning data
set.seed(1)
index <- createDataPartition(dat$Loan_Status, p=0.75, list=FALSE) # has balanced outcome, so not recommended to use sample() function
tr <- dat[index,]
te <- dat[-index,]
## 4. Feature selection rfe = recursive feature elimination
set.seed(1)
RFE <- rfe(x = tr %>% select(-Loan_Status, - Loan_ID),
y = tr$Loan_Status,
rfeControl = rfeControl(functions = rfFuncs, method = "repeatedcv",
repeats = 3, verbose = FALSE))
x <- RFE$optVariables[1:5]
y <- "Loan_Status"
## 5. ML tools in caret
names(getModelInfo())
# Random forest
set.seed(1)
# Random forest
set.seed(1)
model.rf <- train(tr[, x], tr[, y], method = "rf")
## 8. Prediction
pred <- predict.train(object = model.rf, te[, x], type = "raw")
# Random forest
set.seed(1)
model.rf <- train(tr[, x], tr[, y], method = "rf")
# Random forest
set.seed(1)
model.rf <- train(tr[, x], tr[, y], method = "rf")
## 8. Prediction
pred <- predict.train(object = model.rf, te[, x], type = "raw")
table(pred)
confusionMatrix(pred, te[, y])
?reshape2
??reshape2
# Problem 1
WorldPhones %>%
data.frame() %>%
add_rownames("year") %>%
mutate(year = as.numeric(year)) %>%
gather(key = continent, value = phones, - year)  -> dat
lm.1 <- dat %>%
lm(phones ~ year * continent, .)
add_predictions(lm.1)
?add_predictions
library(modelr)
add_predictions(lm.1)
dat %>%
add_predictions(lm.1)
dat %>%
add_predictions(lm.1) %>%
ggplot(aes(year, phones, col = continent)) +
geom_point() +
geom_line(aes(year, pred, col = continent)) +
ylab("Number of phones")
# Problem 2
attitude
knn.tab <- data_frame(k = 1:10, wss = map_dbl(k, ~kmeans(attitude, .)$tot.withinss))
knn.tab
knn.tab %>%
ggplot(aes(k, wss)) +
geom_line()
knn.tab %>%
ggplot(aes(k, wss)) +
geom_line() +
geom_point()
ggplot(cluster_fit, aes(num_centers, wss)) +
geom_point() +
geom_line() +
labs(x="Number of Clusters",
y="Within groups sum of squares")
attitude
km <- kmeans(attitude, 3)
km
attitude %>%
mutate(cluster = as.factor(km$cluster)) %>%
ggplot(aes(learning, complaints, col = cluster)) +
geom_point()
attitude %>%
mutate(cluster = as.factor(km$cluster)) %>%
ggplot(aes(learning, complaints, col = cluster)) +
geom_point() +
geom_point(data= as.data.frame(km$centers),
aes(learning, complaints), col = "black", size=3)
x <- c(1, 2, 3)
class(x)
x[1]
x[2]
x[1,]
length(x)
dim(x)
lalala <- function(x) {
out <- vector("character", length(x))
for (i in seq_along(x)) {
if (x[i] == 1) {
out[i] <- "hello"
} else if (x[i] == 2) {
out[i] <- "world"
} else {out[i] <- 'NA'}
}
}
x <- c(1, 2, 3)
x[x %in% c(1,2,4,5)]
is.na(iris)
any(is.na(iris))
qqnorm(x)
qqplot(x)
qqnorm(x)
qqline(x)
